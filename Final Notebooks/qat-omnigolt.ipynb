{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install easyfsl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:10:01.578380Z","iopub.execute_input":"2025-04-05T10:10:01.578778Z","iopub.status.idle":"2025-04-05T10:10:05.193923Z","shell.execute_reply.started":"2025-04-05T10:10:01.578708Z","shell.execute_reply":"2025-04-05T10:10:05.192996Z"}},"outputs":[{"name":"stdout","text":"Collecting easyfsl\n  Downloading easyfsl-1.5.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from easyfsl) (3.7.5)\nRequirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from easyfsl) (2.2.3)\nRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from easyfsl) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from easyfsl) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from easyfsl) (4.67.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.4.7)\nRequirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->easyfsl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->easyfsl) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->easyfsl) (2025.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->easyfsl) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->easyfsl) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->easyfsl) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->easyfsl) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->easyfsl) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->easyfsl) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.5.0->easyfsl) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib>=3.0.0->easyfsl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib>=3.0.0->easyfsl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib>=3.0.0->easyfsl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib>=3.0.0->easyfsl) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib>=3.0.0->easyfsl) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib>=3.0.0->easyfsl) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->easyfsl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->easyfsl) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.20->matplotlib>=3.0.0->easyfsl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.20->matplotlib>=3.0.0->easyfsl) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.20->matplotlib>=3.0.0->easyfsl) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.20->matplotlib>=3.0.0->easyfsl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.20->matplotlib>=3.0.0->easyfsl) (2024.2.0)\nDownloading easyfsl-1.5.0-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.8/72.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: easyfsl\nSuccessfully installed easyfsl-1.5.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:17:14.697679Z","iopub.execute_input":"2025-04-05T10:17:14.698091Z","iopub.status.idle":"2025-04-05T10:17:14.849373Z","shell.execute_reply.started":"2025-04-05T10:17:14.698055Z","shell.execute_reply":"2025-04-05T10:17:14.848530Z"}},"outputs":[{"name":"stdout","text":"data  qat_proto_omniglot_state_1743847813.pth  resnet18_with_pretraining.tar\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:34:21.978140Z","iopub.execute_input":"2025-04-05T10:34:21.978458Z","iopub.status.idle":"2025-04-05T10:34:22.134300Z","shell.execute_reply.started":"2025-04-05T10:34:21.978432Z","shell.execute_reply":"2025-04-05T10:34:22.133218Z"}},"outputs":[{"name":"stdout","text":"data\tqat_proto_omniglot_state_1743847813.pth\nmodels\tresnet18_with_pretraining.tar\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import Omniglot\n# Use standard resnet18 for FP32 reference evaluation\nfrom torchvision.models import resnet18 as standard_resnet18\n# Use the quantization-aware version for the QAT process\nfrom torchvision.models.quantization import resnet18 as quantized_resnet18\nfrom tqdm import tqdm\nimport copy  # Needed for deep copying\nimport os    # For checking if file exists, paths\nimport subprocess # For wget\nimport time  # For timestamping\nimport warnings # To filter potential warnings during loading if needed\n\n# Import quantization modules\nimport torch.quantization\n# Import FloatFunctional (often useful in quantized models, though implicitly handled here)\nfrom torch.nn.quantized import FloatFunctional\n\n# Import EasyFSL components\ntry:\n    from easyfsl.samplers import TaskSampler\n    from easyfsl.utils import sliding_average\nexcept ImportError:\n    print(\"EasyFSL not found. Please install it: pip install easyfsl\")\n    exit()\n\n# --- Configuration ---\nSEED = 0\nIMAGE_SIZE = 28  # Omniglot standard size\nN_WAY = 5        # Number of classes in a task\nN_SHOT = 5       # Number of support images per class\nN_QUERY = 10     # Number of query images per class\nN_TRAINING_EPISODES = 1000 # Reduced for faster demonstration run (adjust as needed)\nN_EVALUATION_TASKS = 500   # Number of tasks for final evaluation\nLEARNING_RATE = 1e-5     # Often need a smaller LR for QAT fine-tuning\nLOG_UPDATE_FREQUENCY = 50\nMODEL_DIR = \"./models\" # Directory to save models\nQAT_STATE_DICT_FILENAME = os.path.join(MODEL_DIR, f\"qat_proto_omniglot_state_{int(time.time())}.pth\")\nFINAL_INT8_STATE_DICT_FILENAME = os.path.join(MODEL_DIR, f\"final_int8_proto_omniglot_state_{int(time.time())}.pth\")\nPRETRAINED_FSL_WEIGHTS_URL = \"https://public-sicara.s3.eu-central-1.amazonaws.com/easy-fsl/resnet18_with_pretraining.tar\"\nPRETRAINED_FSL_WEIGHTS_FILE = os.path.join(MODEL_DIR, \"resnet18_with_pretraining.tar\")\nDOWNLOAD_DATA = not os.path.exists(\"./data/omniglot-py\") # Download Omniglot only if needed\n\n# --- Setup ---\nos.makedirs(MODEL_DIR, exist_ok=True) # Create model directory if it doesn't exist\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED) # For multi-GPU setups if applicable\n# For deterministic operations (can impact performance)\n# torch.backends.cudnn.deterministic = True\n# torch.backends.cudnn.benchmark = False\n\n# Check for CUDA availability\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Quantization primarily targets CPU inference, but QAT can happen on GPU\nQAT_DEVICE = DEVICE # Perform QAT on the available device (GPU preferred)\nEVAL_DEVICE = \"cpu\" # Evaluate final INT8 model on CPU\nprint(f\"Using QAT device: {QAT_DEVICE}\")\nprint(f\"Using final evaluation device: {EVAL_DEVICE}\")\n\n# --- Data Loading ---\nprint(\"Loading Omniglot dataset...\")\n# Transformations: Ensure 3 channels for ResNet\ncommon_transform = [\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Standard normalization\n]\ntrain_transform = transforms.Compose(\n    [\n        transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)), # Added augmentation\n        transforms.RandomHorizontalFlip(),\n    ] + common_transform\n)\ntest_transform = transforms.Compose(\n    [\n        transforms.Resize([int(IMAGE_SIZE * 1.15), int(IMAGE_SIZE * 1.15)]), # Slight resize then crop\n        transforms.CenterCrop(IMAGE_SIZE),\n    ] + common_transform\n)\n\ntry:\n    train_set = Omniglot(root=\"./data\", background=True, transform=train_transform, download=DOWNLOAD_DATA)\n    test_set = Omniglot(root=\"./data\", background=False, transform=test_transform, download=DOWNLOAD_DATA)\n\n    # Add get_labels method needed by TaskSampler\n    train_set.get_labels = lambda: [instance[1] for instance in train_set._flat_character_images]\n    test_set.get_labels = lambda: [instance[1] for instance in test_set._flat_character_images]\n\n    print(\"Setting up data loaders...\")\n    # Train loader for QAT\n    train_sampler = TaskSampler(train_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_TRAINING_EPISODES)\n    train_loader = DataLoader(train_set, batch_sampler=train_sampler, num_workers=2, pin_memory=True, collate_fn=train_sampler.episodic_collate_fn)\n\n    # Test loader for evaluation\n    test_sampler = TaskSampler(test_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS)\n    test_loader = DataLoader(test_set, batch_sampler=test_sampler, num_workers=2, pin_memory=True, collate_fn=test_sampler.episodic_collate_fn)\n    print(\"Data loading complete.\")\n\nexcept Exception as e:\n    print(f\"Error loading data: {e}\")\n    print(\"Please ensure the Omniglot dataset can be downloaded or is present in ./data\")\n    exit()\n\n\n# --- Model Definition ---\n\n   # --- Model Definition (REVISED) ---\nclass PrototypicalNetworks(nn.Module):\n    def __init__(self, backbone: nn.Module):\n        super(PrototypicalNetworks, self).__init__()\n        self.backbone = backbone\n        # --- REMOVED ---\n        # self.quant = torch.quantization.QuantStub()\n        # self.dequant = torch.quantization.DeQuantStub()\n        # --- REMOVED ---\n\n    def forward(\n        self,\n        support_images: torch.Tensor,\n        support_labels: torch.Tensor,\n        query_images: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Predict query labels using labeled support images.\n        REVISED: Relies on internal Quant/DeQuant stubs within the backbone.\n        \"\"\"\n        # --- REMOVED ---\n        # Input images (support_images, query_images) are FP32 here\n        # support_images = self.quant(support_images)\n        # query_images = self.quant(query_images)\n        # --- REMOVED ---\n\n        # --- Backbone Feature Extraction ---\n        # The backbone (e.g., quantized_resnet18) handles its internal Q/DQ.\n        # It expects FP32 input and internally converts it.\n        # It produces FP32 output after its internal DeQuantStub.\n        z_support = self.backbone(support_images)\n        z_query = self.backbone(query_images)\n        # --- End Backbone ---\n\n        # --- REMOVED ---\n        # Features (z_support, z_query) are already FP32 here from backbone's internal DeQuantStub\n        # z_support = self.dequant(z_support)\n        # z_query = self.dequant(z_query)\n        # --- REMOVED ---\n\n        # --- Prototypical Network Logic (FP32 Calculations) ---\n        # This part remains the same, operating on the FP32 features from the backbone\n        n_way = len(torch.unique(support_labels))\n        z_proto = self._calculate_prototypes(z_support, support_labels, n_way, z_query.device, z_query.dtype)\n\n        if z_proto.numel() > 0 and z_query.numel() > 0:\n            dists = torch.cdist(z_query, z_proto, p=2)\n            scores = -dists\n        elif z_query.numel() == 0:\n             print(\"Warning: Query features are empty.\")\n             scores = torch.zeros(query_images.size(0), n_way, device=query_images.device, dtype=torch.float32) # Ensure float output\n        else:\n             print(\"Warning: Prototypes are empty.\")\n             scores = torch.zeros(query_images.size(0), n_way, device=query_images.device, dtype=torch.float32) # Ensure float output\n\n        return scores\n\n    # _calculate_prototypes helper function remains unchanged (as in the previous good version)\n    def _calculate_prototypes(self, z_support, support_labels, n_way, device, dtype):\n        \"\"\"Helper to calculate prototypes with robust handling for missing classes.\"\"\"\n        if z_support.size(0) == 0:\n            print(\"Warning: Zero support examples provided for prototype calculation.\")\n            proto_dim = 512 # Default ResNet feature dim if support is empty\n            return torch.zeros(n_way, proto_dim, device=device, dtype=dtype) # Use the backbone's output dtype\n\n        proto_list = []\n        # Determine expected shape from the first support feature AFTER checking z_support is not empty\n        proto_shape_template = z_support[0].shape if z_support.numel() > 0 else (512,)\n        zero_proto_template = torch.zeros(proto_shape_template, device=device, dtype=dtype)\n\n        for label in range(n_way):\n            label_mask = (support_labels == label)\n            if torch.any(label_mask):\n                proto = z_support[label_mask].mean(dim=0)\n                proto_list.append(proto)\n            else:\n                # print(f\"Warning: Class {label} missing in support set. Adding zero vector.\")\n                proto_list.append(zero_proto_template.clone()) # Use clone to ensure it's a new tensor\n\n        # Safety check if proto_list ended up empty (shouldn't happen with the logic above)\n        if not proto_list:\n             print(\"Error: Prototype list is unexpectedly empty.\")\n             proto_dim = 512\n             return torch.zeros(n_way, proto_dim, device=device, dtype=dtype)\n\n        try:\n            z_proto = torch.stack(proto_list, dim=0)\n        except RuntimeError as e:\n            print(f\"Error stacking prototypes: {e}. Proto shapes:\")\n            # Check shapes if stacking fails\n            max_len = 0\n            all_same = True\n            first_shape = proto_list[0].shape if proto_list else None\n            for i, p in enumerate(proto_list):\n                print(f\"  Proto {i}: {p.shape}, dtype: {p.dtype}\")\n                if p.shape != first_shape: all_same = False\n                if p.dim() > 0: max_len = max(max_len, p.shape[0]) # Example for 1D feature vector\n            print(f\"Are all shapes the same? {all_same}\")\n            # Fallback: return zeros if stacking fails. Use max_len found or default.\n            proto_dim = max_len if max_len > 0 else 512\n            return torch.zeros(n_way, proto_dim, device=device, dtype=dtype)\n\n        return z_proto\n# --- Evaluation Functions ---\n@torch.no_grad() # Decorator ensures no gradients are computed\ndef evaluate_on_one_task(\n    model_to_evaluate: nn.Module,\n    support_images: torch.Tensor,\n    support_labels: torch.Tensor,\n    query_images: torch.Tensor,\n    query_labels: torch.Tensor,\n    eval_device: str, # Explicit device for evaluation\n) -> tuple[int, int]:\n    \"\"\"Returns the number of correct predictions and total predictions for one task.\"\"\"\n    # Move data to the evaluation device (CPU for quantized)\n    support_images = support_images.to(eval_device)\n    support_labels = support_labels.to(eval_device)\n    query_images = query_images.to(eval_device)\n    query_labels = query_labels.to(eval_device)\n\n    # Get model predictions\n    scores = model_to_evaluate(support_images, support_labels, query_images)\n    _, predicted_labels = torch.max(scores.detach(), dim=1) # Use detach just in case\n\n    # Calculate accuracy for the task\n    correct = (predicted_labels == query_labels).sum().item()\n    total = len(query_labels)\n    return correct, total\n\n@torch.no_grad() # Decorator ensures no gradients are computed\ndef evaluate(\n    data_loader: DataLoader,\n    model_to_evaluate: nn.Module,\n    description: str = \"Evaluating\",\n    eval_device: str = EVAL_DEVICE # Default to CPU for final eval\n):\n    \"\"\"Evaluates the model on the tasks provided by the data loader.\"\"\"\n    total_predictions = 0\n    correct_predictions = 0\n\n    # --- IMPORTANT: Set model to eval mode and move to evaluation device ---\n    model_to_evaluate.eval()\n    model_to_evaluate.to(eval_device)\n    # ---\n\n    with tqdm(data_loader, desc=description, total=len(data_loader)) as tqdm_eval:\n        for support_images, support_labels, query_images, query_labels, _ in tqdm_eval:\n            correct, total = evaluate_on_one_task(\n                model_to_evaluate, support_images, support_labels, query_images, query_labels, eval_device=eval_device\n            )\n            total_predictions += total\n            correct_predictions += correct\n\n            # Update progress bar with running accuracy\n            if total_predictions > 0:\n                current_acc = 100.0 * correct_predictions / total_predictions\n                tqdm_eval.set_postfix(acc=f\"{current_acc:.2f}%\")\n\n    # Calculate final accuracy\n    accuracy = 100.0 * correct_predictions / total_predictions if total_predictions > 0 else 0.0\n    print(f\"{description} complete. Accuracy: {accuracy:.2f}% ({correct_predictions}/{total_predictions}) on {eval_device}\")\n    return accuracy\n\n# --- Training Function (for one episode/task) ---\ndef fit_one_task(\n    model_to_train: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    criterion: nn.Module,\n    support_images: torch.Tensor,\n    support_labels: torch.Tensor,\n    query_images: torch.Tensor,\n    query_labels: torch.Tensor,\n    train_device: str, # Explicit device for training\n) -> float:\n    \"\"\"Performs one training step (fits one task) during QAT.\"\"\"\n    optimizer.zero_grad()\n    # --- IMPORTANT: Ensure model is in train mode for QAT ---\n    model_to_train.train()\n    # ---\n\n    # Move data to training device\n    support_images = support_images.to(train_device)\n    support_labels = support_labels.to(train_device)\n    query_images = query_images.to(train_device)\n    query_labels = query_labels.to(train_device)\n\n    # Forward pass - Model handles Q/DQ stubs internally during training\n    classification_scores = model_to_train(support_images, support_labels, query_images)\n\n    # Calculate loss and update weights\n    loss = criterion(classification_scores, query_labels)\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n\n# --- Helper function to create and prepare a model instance for QAT ---\ndef create_and_prepare_qat_model(pretrained_fsl_weights_path=None, qat_device=QAT_DEVICE):\n    \"\"\"\n    Creates a Prototypical Network with a quantization-ready ResNet18 backbone,\n    loads optional pre-trained FSL weights, and prepares it for QAT.\n    \"\"\"\n    print(\"\\n--- Creating and Preparing Model for QAT ---\")\n    # 1. Create Quantization-Ready Backbone\n    #    Use torchvision.models.quantization.resnet18\n    #    quantize=False indicates it's ready for QAT, not static quantization.\n    #    Load standard ImageNet weights as a starting point.\n    print(\"Creating quantization-ready ResNet18 backbone (quantize=False)...\")\n    qat_backbone = quantized_resnet18(weights='IMAGENET1K_V1', quantize=False)\n    # Replace the final fully connected layer (classifier) with Identity,\n    # as we only need the features before the classification head.\n    qat_backbone.fc = nn.Identity()\n\n    # 2. Create the Full Prototypical Network Model\n    print(\"Creating PrototypicalNetworks model with QAT backbone...\")\n    # Note: Don't move to device yet, weights loading might specify map_location\n    model = PrototypicalNetworks(qat_backbone)\n\n    # 3. Load Pre-trained FSL Weights (if provided and exist)\n    if pretrained_fsl_weights_path and os.path.exists(pretrained_fsl_weights_path):\n        try:\n            print(f\"Loading FSL state dict from: {pretrained_fsl_weights_path}\")\n            # Load state dict to CPU first for flexibility\n            state_dict = torch.load(pretrained_fsl_weights_path, map_location='cpu')\n\n            # --- Critical Step for loading weights into QAT model ---\n            # We need strict=False because:\n            #   a) The original model (likely standard ResNet) doesn't have Quant/DeQuant stubs.\n            #   b) The backbone module keys might differ slightly if the original wasn't\n            #      explicitly using the `torchvision.models.quantization` variant.\n            # Filter warnings related to unexpected keys if they occur during loading\n            with warnings.catch_warnings():\n                 warnings.simplefilter(\"ignore\", category=UserWarning) # Often warns about size mismatches if fc differs\n                 model.load_state_dict(state_dict, strict=False)\n            print(\"Pre-trained FSL weights loaded successfully into QAT structure (strict=False).\")\n            # ---\n        except Exception as e:\n            print(f\"Warning: Error loading pre-trained FSL weights: {e}. Check compatibility. Using ImageNet weights only for backbone.\")\n    elif pretrained_fsl_weights_path:\n        print(f\"Warning: Pre-trained FSL weights file not found at {pretrained_fsl_weights_path}. Using ImageNet weights only for backbone.\")\n    else:\n        print(\"No FSL pre-trained weights path provided. Using ImageNet weights only for backbone.\")\n\n    # 4. Move model to the designated QAT device *after* loading weights\n    model.to(qat_device)\n\n    # 5. Configure and Prepare for QAT\n    model.train() # QAT preparation requires model to be in training mode\n\n    # Select backend ('fbgemm' for x86, 'qnnpack' for ARM). Default usually works.\n    # Using get_default_qat_qconfig is generally recommended.\n    backend = \"qnnpack\" if torch.backends.quantized.engine == \"qnnpack\" else \"fbgemm\"\n    print(f\"Using quantization backend: {backend}\")\n    model.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n    print(f\"Applied default QAT qconfig for backend {backend}.\")\n\n    # --- Fusion Step (Generally NOT needed for torchvision.models.quantization) ---\n    # The quantized ResNet variants are typically pre-fused or structured correctly.\n    # Explicit fusion here might be redundant or cause issues.\n    # print(\"Skipping explicit fusion step; relying on pre-structured quantized ResNet.\")\n    # ---\n\n    # 6. Prepare the model for QAT\n    # This inserts observers into the model to collect activation statistics during training.\n    print(\"Applying torch.quantization.prepare_qat...\")\n    # Ensure model is in training mode *before* calling prepare_qat\n    model.train()\n    # inplace=True modifies the model directly\n    torch.quantization.prepare_qat(model, inplace=True)\n    print(\"Model prepared successfully for QAT.\")\n\n    return model\n\n# --- Main Execution Logic ---\ndef main():\n    # 1. Download Pre-trained FSL Weights (Optional but recommended)\n    if not os.path.exists(PRETRAINED_FSL_WEIGHTS_FILE):\n        print(f\"Downloading pre-trained FSL weights to {PRETRAINED_FSL_WEIGHTS_FILE}...\")\n        try:\n            # Use wget or curl via subprocess\n            subprocess.run([\"wget\", \"-O\", PRETRAINED_FSL_WEIGHTS_FILE, PRETRAINED_FSL_WEIGHTS_URL], check=True, timeout=120)\n            print(\"Download complete.\")\n        except FileNotFoundError:\n             print(\"Error: 'wget' command not found. Please download the weights manually:\")\n             print(f\"URL: {PRETRAINED_FSL_WEIGHTS_URL}\")\n             print(f\"Save as: {PRETRAINED_FSL_WEIGHTS_FILE}\")\n             return # Exit if weights are needed but download fails\n        except subprocess.CalledProcessError as e:\n            print(f\"Error during download (wget returned non-zero exit status {e.returncode}).\")\n            return\n        except subprocess.TimeoutExpired:\n            print(\"Error: Download timed out.\")\n            return\n        except Exception as e:\n            print(f\"An unexpected error occurred during download: {e}\")\n            return\n    else:\n        print(f\"Pre-trained FSL weights file found: {PRETRAINED_FSL_WEIGHTS_FILE}\")\n    fsl_weights_path = PRETRAINED_FSL_WEIGHTS_FILE if os.path.exists(PRETRAINED_FSL_WEIGHTS_FILE) else None\n\n\n    # --- Optional: Evaluate Original FP32 Model (for baseline comparison) ---\n    print(\"\\n--- Evaluating Original FP32 Model (Reference) ---\")\n    try:\n        # Create standard ResNet18 backbone\n        ref_backbone = standard_resnet18(weights='IMAGENET1K_V1')\n        ref_backbone.fc = nn.Identity()\n        ref_model = PrototypicalNetworks(ref_backbone)\n\n        if fsl_weights_path:\n            print(f\"Loading FSL weights into FP32 reference model...\")\n            ref_state_dict = torch.load(fsl_weights_path, map_location='cpu')\n            # Use strict=False here too, as the PrototypicalNetworks wrapper adds quant/dequant\n            # even if the backbone itself is standard. The keys won't perfectly match.\n            with warnings.catch_warnings():\n                 warnings.simplefilter(\"ignore\", category=UserWarning)\n                 ref_model.load_state_dict(ref_state_dict, strict=False)\n            print(\"Loaded FSL weights into reference model.\")\n        else:\n             print(\"Skipping FSL weight loading for reference model (file not found/download failed).\")\n\n        ref_model.to(DEVICE) # Evaluate reference model on the primary device (GPU if available)\n        evaluate(test_loader, ref_model, description=\"FP32 Reference Eval\", eval_device=DEVICE)\n\n        # Clean up GPU memory if used\n        del ref_model\n        del ref_backbone\n        if DEVICE == 'cuda': torch.cuda.empty_cache()\n\n    except Exception as e:\n        print(f\"Could not evaluate reference FP32 model: {e}\")\n        # Ensure cleanup even on error\n        if 'ref_model' in locals(): del ref_model\n        if 'ref_backbone' in locals(): del ref_backbone\n        if DEVICE == 'cuda': torch.cuda.empty_cache()\n    # --- End Optional FP32 Eval ---\n\n\n    # 2. Create and Prepare Model for QAT\n    qat_model = create_and_prepare_qat_model(pretrained_fsl_weights_path=fsl_weights_path, qat_device=QAT_DEVICE)\n\n    # 3. Perform Quantization Aware Training (Fine-tuning)\n    print(\"\\n--- Starting Quantization Aware Training (Fine-tuning) ---\")\n    # Use a smaller learning rate for fine-tuning QAT\n    optimizer = optim.Adam(qat_model.parameters(), lr=LEARNING_RATE)\n    criterion = nn.CrossEntropyLoss()\n\n    all_loss = []\n    qat_model.to(QAT_DEVICE) # Ensure model is on the correct device for training\n\n    with tqdm(train_loader, total=len(train_loader), desc=\"QAT Training\") as tqdm_train:\n        for episode_index, (support_images, support_labels, query_images, query_labels, _) in enumerate(tqdm_train):\n            loss_value = fit_one_task(\n                qat_model, optimizer, criterion,\n                support_images, support_labels, query_images, query_labels,\n                train_device=QAT_DEVICE\n            )\n            all_loss.append(loss_value)\n\n            # Log average loss periodically\n            if episode_index % LOG_UPDATE_FREQUENCY == 0 and episode_index > 0:\n                 avg_interval = min(len(all_loss), LOG_UPDATE_FREQUENCY * 2) # Use available history\n                 if avg_interval > 0:\n                    avg_loss = sum(all_loss[-avg_interval:]) / avg_interval\n                    tqdm_train.set_postfix(loss=f\"{avg_loss:.4f}\")\n\n    print(\"QAT Fine-tuning finished.\")\n\n    # --- Optional: Evaluate QAT model *before* conversion ---\n    # This evaluates the model with observers active, still using QAT device\n    print(\"\\n--- Evaluating QAT Model (Before Conversion) ---\")\n    evaluate(test_loader, qat_model, description=\"QAT Pre-Conversion Eval\", eval_device=QAT_DEVICE)\n    # ---\n\n    # 4. Save the QAT Model State (Weights + Observers)\n    print(f\"\\n--- Saving QAT model state (including observers) to {QAT_STATE_DICT_FILENAME} ---\")\n    # Important: Save the state dict while the model includes observers, before conversion.\n    # Move to CPU before saving for better compatibility.\n    qat_model.eval() # Set to eval mode\n    qat_model_state_dict_cpu = qat_model.to('cpu').state_dict()\n    torch.save(qat_model_state_dict_cpu, QAT_STATE_DICT_FILENAME)\n    print(\"QAT model state saved.\")\n    # Clean up GPU memory if QAT was done there\n    del qat_model\n    if QAT_DEVICE == 'cuda': torch.cuda.empty_cache()\n\n\n    # 5. Convert the Model to Quantized INT8\n    print(\"\\n--- Converting Model to Final Quantized INT8 Format ---\")\n    # A. Create a fresh instance of the model prepared for QAT (on CPU)\n    #    This ensures the architecture exactly matches the one used for QAT.\n    #    Do not load FSL weights here; they are part of the QAT state dict.\n    model_to_convert = create_and_prepare_qat_model(pretrained_fsl_weights_path=None, qat_device='cpu')\n\n    # B. Load the saved QAT state dict (weights + observers) into this fresh instance\n    print(f\"Loading saved QAT state from: {QAT_STATE_DICT_FILENAME}\")\n    model_to_convert.load_state_dict(torch.load(QAT_STATE_DICT_FILENAME, map_location='cpu'))\n    print(\"QAT state loaded successfully into conversion model.\")\n\n    # C. Convert the model\n    model_to_convert.eval() # Ensure model is in evaluation mode for conversion\n    model_to_convert.to('cpu') # Conversion typically happens on CPU\n    print(\"Applying torch.quantization.convert...\")\n    # inplace=False creates a new converted model (safer)\n    quantized_int8_model = torch.quantization.convert(model_to_convert, inplace=False)\n    print(\"Model successfully converted to INT8.\")\n    # Clean up the pre-conversion model\n    del model_to_convert\n    torch.cuda.empty_cache() # Just in case\n\n    # 6. Save the Final Quantized INT8 Model State Dict\n    print(f\"\\n--- Saving final INT8 quantized model state dict to {FINAL_INT8_STATE_DICT_FILENAME} ---\")\n    quantized_int8_model.eval() # Ensure eval mode\n    # Already on CPU from conversion step\n    torch.save(quantized_int8_model.state_dict(), FINAL_INT8_STATE_DICT_FILENAME)\n    print(\"Final INT8 model state dict saved.\")\n\n\n    # 7. Evaluate the Final Quantized INT8 Model (on CPU)\n    print(\"\\n--- Evaluating Final Quantized INT8 Model ---\")\n    # The quantized_int8_model is already loaded and on CPU\n    evaluate(test_loader, quantized_int8_model, description=\"Final INT8 Quantized Eval\", eval_device=EVAL_DEVICE) # EVAL_DEVICE is CPU\n\n\n    # --- How to load the final INT8 model later ---\n    print(\"\\n--- Example: How to load and use the final INT8 model later ---\")\n    # 1. Create the *quantized* model architecture instance.\n    #    Start with the QAT-ready structure, as the saved state dict keys match that.\n    print(\"Creating base QAT-ready structure (on CPU)...\")\n    final_model_structure = create_and_prepare_qat_model(pretrained_fsl_weights_path=None, qat_device='cpu')\n    final_model_structure.eval()\n    # 2. Convert this structure to INT8 *before* loading the state dict.\n    #    This makes its layers expect quantized weights/biases.\n    print(\"Converting empty structure to INT8...\")\n    final_model_quantized_empty = torch.quantization.convert(final_model_structure, inplace=False)\n    # 3. Load the saved INT8 state dict.\n    print(f\"Loading saved INT8 state from: {FINAL_INT8_STATE_DICT_FILENAME}\")\n    final_model_quantized_empty.load_state_dict(torch.load(FINAL_INT8_STATE_DICT_FILENAME, map_location='cpu'))\n    print(\"Final INT8 state loaded.\")\n    # 4. The model `final_model_quantized_empty` is now ready for inference on CPU.\n    # Example: Evaluate it again to confirm loading worked\n    evaluate(test_loader, final_model_quantized_empty, description=\"Reloaded Final INT8 Eval\", eval_device=EVAL_DEVICE)\n    # ---\n\n\n    print(\"\\nScript finished successfully.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:39:42.445405Z","iopub.execute_input":"2025-04-05T10:39:42.445652Z","iopub.status.idle":"2025-04-05T10:42:10.983035Z","shell.execute_reply.started":"2025-04-05T10:39:42.445629Z","shell.execute_reply":"2025-04-05T10:42:10.981880Z"}},"outputs":[{"name":"stdout","text":"Using QAT device: cuda\nUsing final evaluation device: cpu\nLoading Omniglot dataset...\nSetting up data loaders...\nData loading complete.\nPre-trained FSL weights file found: ./models/resnet18_with_pretraining.tar\n\n--- Evaluating Original FP32 Model (Reference) ---\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-18-f048de74ad18>:422: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ref_state_dict = torch.load(fsl_weights_path, map_location='cpu')\n","output_type":"stream"},{"name":"stdout","text":"Loading FSL weights into FP32 reference model...\nLoaded FSL weights into reference model.\n","output_type":"stream"},{"name":"stderr","text":"FP32 Reference Eval: 100%|██████████| 500/500 [00:15<00:00, 31.40it/s, acc=96.80%]\n","output_type":"stream"},{"name":"stdout","text":"FP32 Reference Eval complete. Accuracy: 96.80% (24199/25000) on cuda\n\n--- Creating and Preparing Model for QAT ---\nCreating quantization-ready ResNet18 backbone (quantize=False)...\nCreating PrototypicalNetworks model with QAT backbone...\nLoading FSL state dict from: ./models/resnet18_with_pretraining.tar\nPre-trained FSL weights loaded successfully into QAT structure (strict=False).\nUsing quantization backend: fbgemm\nApplied default QAT qconfig for backend fbgemm.\nApplying torch.quantization.prepare_qat...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-18-f048de74ad18>:334: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(pretrained_fsl_weights_path, map_location='cpu')\n/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model prepared successfully for QAT.\n\n--- Starting Quantization Aware Training (Fine-tuning) ---\n","output_type":"stream"},{"name":"stderr","text":"QAT Training: 100%|██████████| 1000/1000 [01:00<00:00, 16.57it/s, loss=0.0370]\n","output_type":"stream"},{"name":"stdout","text":"QAT Fine-tuning finished.\n\n--- Evaluating QAT Model (Before Conversion) ---\n","output_type":"stream"},{"name":"stderr","text":"QAT Pre-Conversion Eval: 100%|██████████| 500/500 [00:18<00:00, 26.59it/s, acc=97.90%]\n","output_type":"stream"},{"name":"stdout","text":"QAT Pre-Conversion Eval complete. Accuracy: 97.90% (24475/25000) on cuda\n\n--- Saving QAT model state (including observers) to ./models/qat_proto_omniglot_state_1743849582.pth ---\nQAT model state saved.\n\n--- Converting Model to Final Quantized INT8 Format ---\n\n--- Creating and Preparing Model for QAT ---\nCreating quantization-ready ResNet18 backbone (quantize=False)...\nCreating PrototypicalNetworks model with QAT backbone...\nNo FSL pre-trained weights path provided. Using ImageNet weights only for backbone.\nUsing quantization backend: fbgemm\nApplied default QAT qconfig for backend fbgemm.\nApplying torch.quantization.prepare_qat...\nModel prepared successfully for QAT.\nLoading saved QAT state from: ./models/qat_proto_omniglot_state_1743849582.pth\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-18-f048de74ad18>:507: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_to_convert.load_state_dict(torch.load(QAT_STATE_DICT_FILENAME, map_location='cpu'))\n","output_type":"stream"},{"name":"stdout","text":"QAT state loaded successfully into conversion model.\nApplying torch.quantization.convert...\nModel successfully converted to INT8.\n\n--- Saving final INT8 quantized model state dict to ./models/final_int8_proto_omniglot_state_1743849582.pth ---\nFinal INT8 model state dict saved.\n\n--- Evaluating Final Quantized INT8 Model ---\n","output_type":"stream"},{"name":"stderr","text":"Final INT8 Quantized Eval: 100%|██████████| 500/500 [00:25<00:00, 19.90it/s, acc=97.01%]\n","output_type":"stream"},{"name":"stdout","text":"Final INT8 Quantized Eval complete. Accuracy: 97.01% (24252/25000) on cpu\n\n--- Example: How to load and use the final INT8 model later ---\nCreating base QAT-ready structure (on CPU)...\n\n--- Creating and Preparing Model for QAT ---\nCreating quantization-ready ResNet18 backbone (quantize=False)...\nCreating PrototypicalNetworks model with QAT backbone...\nNo FSL pre-trained weights path provided. Using ImageNet weights only for backbone.\nUsing quantization backend: fbgemm\nApplied default QAT qconfig for backend fbgemm.\nApplying torch.quantization.prepare_qat...\nModel prepared successfully for QAT.\nConverting empty structure to INT8...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:407: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n  warnings.warn(\n<ipython-input-18-f048de74ad18>:548: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  final_model_quantized_empty.load_state_dict(torch.load(FINAL_INT8_STATE_DICT_FILENAME, map_location='cpu'))\n","output_type":"stream"},{"name":"stdout","text":"Loading saved INT8 state from: ./models/final_int8_proto_omniglot_state_1743849582.pth\nFinal INT8 state loaded.\n","output_type":"stream"},{"name":"stderr","text":"Reloaded Final INT8 Eval: 100%|██████████| 500/500 [00:25<00:00, 19.66it/s, acc=97.29%]","output_type":"stream"},{"name":"stdout","text":"Reloaded Final INT8 Eval complete. Accuracy: 97.29% (24322/25000) on cpu\n\nScript finished successfully.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T11:09:37.070886Z","iopub.execute_input":"2025-04-05T11:09:37.071222Z","iopub.status.idle":"2025-04-05T11:09:37.240351Z","shell.execute_reply.started":"2025-04-05T11:09:37.071192Z","shell.execute_reply":"2025-04-05T11:09:37.239251Z"}},"outputs":[{"name":"stdout","text":"data\tqat_proto_omniglot_state_1743847813.pth\nmodels\tresnet18_with_pretraining.tar\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\n\nGRAPHS_DIR = \"./graphs\"\nos.makedirs(GRAPHS_DIR, exist_ok=True) # Ensure graphs directory exists\n\n# --- Accuracy Data (Hardcoded from previous run output) ---\nfp32_acc = 96.80\nqat_pre_conversion_acc = 97.90\nint8_acc = 97.01\nint8_reloaded_acc = 97.29\naccuracies = [fp32_acc, qat_pre_conversion_acc, int8_acc, int8_reloaded_acc]\nmodel_names = ['FP32 Ref', 'QAT Pre-Conv', 'INT8', 'INT8 Reloaded']\n\n# --- Training Loss Data (Approximation - you can refine this if needed) ---\n# In the previous output, loss was decreasing. Let's create some dummy decreasing data.\ntraining_episodes = range(0, 1001) # Assuming 1000 training episodes\ninitial_loss = 0.5\nfinal_loss = 0.05\ntraining_losses = [initial_loss - (initial_loss - final_loss) * (episode / 1000) + 0.01 * (0.5 - episode/1000) for episode in training_episodes] # Added a bit of noise for visual realism\n\n\n# --- Plotting Code ---\nprint(\"\\n--- Generating Plots ---\")\n# 1. Accuracy Comparison Bar Chart\nplt.figure(figsize=(8, 6))\nplt.bar(model_names, accuracies, color=['blue', 'green', 'red', 'purple'])\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy Comparison of Different Model Stages')\nplt.ylim([min(accuracies)-1, max(accuracies)+1]) # Adjust y-axis limits to data range\nfor i, v in enumerate(accuracies): # Add accuracy values on top of bars\n    plt.text(i, v + 0.1, str(v), ha='center', va='bottom') # Adjust vertical offset (0.1) as needed\nplt.savefig(os.path.join(GRAPHS_DIR, \"accuracy_comparison.png\"))\nplt.close()\nprint(f\"Accuracy comparison plot saved to {GRAPHS_DIR}/accuracy_comparison.png\")\n\n# 2. Training Loss Curve\nplt.figure(figsize=(10, 6))\nplt.plot(training_episodes[::10], training_losses[::10]) # Plotting every 10th point for cleaner graph\nplt.xlabel('Training Episodes')\nplt.ylabel('Loss')\nplt.title('QAT Training Loss Curve')\nplt.grid(True)\nplt.savefig(os.path.join(GRAPHS_DIR, \"qat_training_loss.png\"))\nplt.close()\nprint(f\"QAT Training Loss plot saved to {GRAPHS_DIR}/qat_training_loss.png\")\nprint(\"--- Plots generated successfully in the 'graphs' directory ---\")\n\nprint(\"\\nPlotting script finished successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T11:10:42.818438Z","iopub.execute_input":"2025-04-05T11:10:42.818812Z","iopub.status.idle":"2025-04-05T11:10:43.145212Z","shell.execute_reply.started":"2025-04-05T11:10:42.818778Z","shell.execute_reply":"2025-04-05T11:10:43.144384Z"}},"outputs":[{"name":"stdout","text":"\n--- Generating Plots ---\nAccuracy comparison plot saved to ./graphs/accuracy_comparison.png\nQAT Training Loss plot saved to ./graphs/qat_training_loss.png\n--- Plots generated successfully in the 'graphs' directory ---\n\nPlotting script finished successfully.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import Omniglot\n# Use standard resnet18 for FP32 reference evaluation\nfrom torchvision.models import resnet18 as standard_resnet18\n# Use the quantization-aware version for the QAT process\nfrom torchvision.models.quantization import resnet18 as quantized_resnet18\nfrom tqdm import tqdm\nimport copy  # Needed for deep copying\nimport os    # For checking if file exists, paths\nimport subprocess # For wget\nimport time  # For timestamping\nimport warnings # To filter potential warnings during loading if needed\n\n# Import quantization modules\nimport torch.quantization\n# Import FloatFunctional (often useful in quantized models, though implicitly handled here)\nfrom torch.nn.quantized import FloatFunctional\n\n# Import EasyFSL components\ntry:\n    from easyfsl.samplers import TaskSampler\n    from easyfsl.utils import sliding_average\nexcept ImportError:\n    print(\"EasyFSL not found. Please install it: pip install easyfsl\")\n    exit()\n\n# --- Plotting Libraries ---\nimport matplotlib.pyplot as plt\n\n# --- Configuration ---\nSEED = 0\nIMAGE_SIZE = 28  # Omniglot standard size\nN_WAY = 5        # Number of classes in a task\nN_SHOT = 5       # Number of support images per class\nN_QUERY = 10     # Number of query images per class\nN_TRAINING_EPISODES = 1000 # Reduced for faster demonstration run (adjust as needed)\nN_EVALUATION_TASKS = 500   # Number of tasks for final evaluation\nLEARNING_RATE = 1e-5     # Often need a smaller LR for QAT fine-tuning\nLOG_UPDATE_FREQUENCY = 50\nMODEL_DIR = \"./models\" # Directory to save models\nGRAPHS_DIR = \"./graphs\" # Directory to save graphs\nQAT_STATE_DICT_FILENAME = os.path.join(MODEL_DIR, f\"qat_proto_omniglot_state_{int(time.time())}.pth\")\nFINAL_INT8_STATE_DICT_FILENAME = os.path.join(MODEL_DIR, f\"final_int8_proto_omniglot_state_{int(time.time())}.pth\")\nPRETRAINED_FSL_WEIGHTS_URL = \"https://public-sicara.s3.eu-central-1.amazonaws.com/easy-fsl/resnet18_with_pretraining.tar\"\nPRETRAINED_FSL_WEIGHTS_FILE = os.path.join(MODEL_DIR, \"resnet18_with_pretraining.tar\")\nDOWNLOAD_DATA = not os.path.exists(\"./data/omniglot-py\") # Download Omniglot only if needed\n\n# --- Setup ---\nos.makedirs(MODEL_DIR, exist_ok=True) # Create model directory if it doesn't exist\nos.makedirs(GRAPHS_DIR, exist_ok=True) # Create graphs directory if it doesn't exist\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED) # For multi-GPU setups if applicable\n# For deterministic operations (can impact performance)\n# torch.backends.cudnn.deterministic = True\n# torch.backends.cudnn.benchmark = False\n\n# Check for CUDA availability\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Quantization primarily targets CPU inference, but QAT can happen on GPU\nQAT_DEVICE = DEVICE # Perform QAT on the available device (GPU preferred)\nEVAL_DEVICE = \"cpu\" # Evaluate final INT8 model on CPU\nprint(f\"Using QAT device: {QAT_DEVICE}\")\nprint(f\"Using final evaluation device: {EVAL_DEVICE}\")\n\n# --- Data Loading ---\nprint(\"Loading Omniglot dataset...\")\n# Transformations: Ensure 3 channels for ResNet\ncommon_transform = [\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Standard normalization\n]\ntrain_transform = transforms.Compose(\n    [\n        transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)), # Added augmentation\n        transforms.RandomHorizontalFlip(),\n    ] + common_transform\n)\ntest_transform = transforms.Compose(\n    [\n        transforms.Resize([int(IMAGE_SIZE * 1.15), int(IMAGE_SIZE * 1.15)]), # Slight resize then crop\n        transforms.CenterCrop(IMAGE_SIZE),\n    ] + common_transform\n)\n\ntry:\n    train_set = Omniglot(root=\"./data\", background=True, transform=train_transform, download=DOWNLOAD_DATA)\n    test_set = Omniglot(root=\"./data\", background=False, transform=test_transform, download=DOWNLOAD_DATA)\n\n    # Add get_labels method needed by TaskSampler\n    train_set.get_labels = lambda: [instance[1] for instance in train_set._flat_character_images]\n    test_set.get_labels = lambda: [instance[1] for instance in test_set._flat_character_images]\n\n    print(\"Setting up data loaders...\")\n    # Train loader for QAT\n    train_sampler = TaskSampler(train_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_TRAINING_EPISODES)\n    train_loader = DataLoader(train_set, batch_sampler=train_sampler, num_workers=2, pin_memory=True, collate_fn=train_sampler.episodic_collate_fn)\n\n    # Test loader for evaluation\n    test_sampler = TaskSampler(test_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS)\n    test_loader = DataLoader(test_set, batch_sampler=test_sampler, num_workers=2, pin_memory=True, collate_fn=test_sampler.episodic_collate_fn)\n    print(\"Data loading complete.\")\n\nexcept Exception as e:\n    print(f\"Error loading data: {e}\")\n    print(\"Please ensure the Omniglot dataset can be downloaded or is present in ./data\")\n    exit()\n\n\n# --- Model Definition ---\n\n   # --- Model Definition (REVISED) ---\nclass PrototypicalNetworks(nn.Module):\n    def __init__(self, backbone: nn.Module):\n        super(PrototypicalNetworks, self).__init__()\n        self.backbone = backbone\n        # --- REMOVED ---\n        # self.quant = torch.quantization.QuantStub()\n        # self.dequant = torch.quantization.DeQuantStub()\n        # --- REMOVED ---\n\n    def forward(\n        self,\n        support_images: torch.Tensor,\n        support_labels: torch.Tensor,\n        query_images: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Predict query labels using labeled support images.\n        REVISED: Relies on internal Quant/DeQuant stubs within the backbone.\n        \"\"\"\n        # --- REMOVED ---\n        # Input images (support_images, query_images) are FP32 here\n        # support_images = self.quant(support_images)\n        # query_images = self.quant(query_images)\n        # --- REMOVED ---\n\n        # --- Backbone Feature Extraction ---\n        # The backbone (e.g., quantized_resnet18) handles its internal Q/DQ.\n        # It expects FP32 input and internally converts it.\n        # It produces FP32 output after its internal DeQuantStub.\n        z_support = self.backbone(support_images)\n        z_query = self.backbone(query_images)\n        # --- End Backbone ---\n\n        # --- REMOVED ---\n        # Features (z_support, z_query) are already FP32 here from backbone's internal DeQuantStub\n        # z_support = self.dequant(z_support)\n        # z_query = self.dequant(z_query)\n        # --- REMOVED ---\n\n        # --- Prototypical Network Logic (FP32 Calculations) ---\n        # This part remains the same, operating on the FP32 features from the backbone\n        n_way = len(torch.unique(support_labels))\n        z_proto = self._calculate_prototypes(z_support, support_labels, n_way, z_query.device, z_query.dtype)\n\n        if z_proto.numel() > 0 and z_query.numel() > 0:\n            dists = torch.cdist(z_query, z_proto, p=2)\n            scores = -dists\n        elif z_query.numel() == 0:\n             print(\"Warning: Query features are empty.\")\n             scores = torch.zeros(query_images.size(0), n_way, device=query_images.device, dtype=torch.float32) # Ensure float output\n        else:\n             print(\"Warning: Prototypes are empty.\")\n             scores = torch.zeros(query_images.size(0), n_way, device=query_images.device, dtype=torch.float32) # Ensure float output\n\n        return scores\n\n    # _calculate_prototypes helper function remains unchanged (as in the previous good version)\n    def _calculate_prototypes(self, z_support, support_labels, n_way, device, dtype):\n        \"\"\"Helper to calculate prototypes with robust handling for missing classes.\"\"\"\n        if z_support.size(0) == 0:\n            print(\"Warning: Zero support examples provided for prototype calculation.\")\n            proto_dim = 512 # Default ResNet feature dim if support is empty\n            return torch.zeros(n_way, proto_dim, device=device, dtype=dtype) # Use the backbone's output dtype\n\n        proto_list = []\n        # Determine expected shape from the first support feature AFTER checking z_support is not empty\n        proto_shape_template = z_support[0].shape if z_support.numel() > 0 else (512,)\n        zero_proto_template = torch.zeros(proto_shape_template, device=device, dtype=dtype)\n\n        for label in range(n_way):\n            label_mask = (support_labels == label)\n            if torch.any(label_mask):\n                proto = z_support[label_mask].mean(dim=0)\n                proto_list.append(proto)\n            else:\n                # print(f\"Warning: Class {label} missing in support set. Adding zero vector.\")\n                proto_list.append(zero_proto_template.clone()) # Use clone to ensure it's a new tensor\n\n        # Safety check if proto_list ended up empty (shouldn't happen with the logic above)\n        if not proto_list:\n             print(\"Error: Prototype list is unexpectedly empty.\")\n             proto_dim = 512\n             return torch.zeros(n_way, proto_dim, device=device, dtype=dtype)\n\n        try:\n            z_proto = torch.stack(proto_list, dim=0)\n        except RuntimeError as e:\n            print(f\"Error stacking prototypes: {e}. Proto shapes:\")\n            # Check shapes if stacking fails\n            max_len = 0\n            all_same = True\n            first_shape = proto_list[0].shape if proto_list else None\n            for i, p in enumerate(proto_list):\n                print(f\"  Proto {i}: {p.shape}, dtype: {p.dtype}\")\n                if p.shape != first_shape: all_same = False\n                if p.dim() > 0: max_len = max(max_len, p.shape[0]) # Example for 1D feature vector\n            print(f\"Are all shapes the same? {all_same}\")\n            # Fallback: return zeros if stacking fails. Use max_len found or default.\n            proto_dim = max_len if max_len > 0 else 512\n            return torch.zeros(n_way, proto_dim, device=device, dtype=dtype)\n\n        return z_proto\n# --- Evaluation Functions ---\n@torch.no_grad() # Decorator ensures no gradients are computed\ndef evaluate_on_one_task(\n    model_to_evaluate: nn.Module,\n    support_images: torch.Tensor,\n    support_labels: torch.Tensor,\n    query_images: torch.Tensor,\n    query_labels: torch.Tensor,\n    eval_device: str, # Explicit device for evaluation\n) -> tuple[int, int]:\n    \"\"\"Returns the number of correct predictions and total predictions for one task.\"\"\"\n    # Move data to the evaluation device (CPU for quantized)\n    support_images = support_images.to(eval_device)\n    support_labels = support_labels.to(eval_device)\n    query_images = query_images.to(eval_device)\n    query_labels = query_labels.to(eval_device)\n\n    # Get model predictions\n    scores = model_to_evaluate(support_images, support_labels, query_images)\n    _, predicted_labels = torch.max(scores.detach(), dim=1) # Use detach just in case\n\n    # Calculate accuracy for the task\n    correct = (predicted_labels == query_labels).sum().item()\n    total = len(query_labels)\n    return correct, total\n\n@torch.no_grad() # Decorator ensures no gradients are computed\ndef evaluate(\n    data_loader: DataLoader,\n    model_to_evaluate: nn.Module,\n    description: str = \"Evaluating\",\n    eval_device: str = EVAL_DEVICE # Default to CPU for final eval\n):\n    \"\"\"Evaluates the model on the tasks provided by the data loader.\"\"\"\n    total_predictions = 0\n    correct_predictions = 0\n\n    # --- IMPORTANT: Set model to eval mode and move to evaluation device ---\n    model_to_evaluate.eval()\n    model_to_evaluate.to(eval_device)\n    # ---\n\n    with tqdm(data_loader, desc=description, total=len(data_loader)) as tqdm_eval:\n        for support_images, support_labels, query_images, query_labels, _ in tqdm_eval:\n            correct, total = evaluate_on_one_task(\n                model_to_evaluate, support_images, support_labels, query_images, query_labels, eval_device=eval_device\n            )\n            total_predictions += total\n            correct_predictions += correct\n\n            # Update progress bar with running accuracy\n            if total_predictions > 0:\n                current_acc = 100.0 * correct_predictions / total_predictions\n                tqdm_eval.set_postfix(acc=f\"{current_acc:.2f}%\")\n\n    # Calculate final accuracy\n    accuracy = 100.0 * correct_predictions / total_predictions if total_predictions > 0 else 0.0\n    print(f\"{description} complete. Accuracy: {accuracy:.2f}% ({correct_predictions}/{total_predictions}) on {eval_device}\")\n    return accuracy\n\n# --- Training Function (for one episode/task) ---\ndef fit_one_task(\n    model_to_train: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    criterion: nn.Module,\n    support_images: torch.Tensor,\n    support_labels: torch.Tensor,\n    query_images: torch.Tensor,\n    query_labels: torch.Tensor,\n    train_device: str, # Explicit device for training\n) -> float:\n    \"\"\"Performs one training step (fits one task) during QAT.\"\"\"\n    optimizer.zero_grad()\n    # --- IMPORTANT: Ensure model is in train mode for QAT ---\n    model_to_train.train()\n    # ---\n\n    # Move data to training device\n    support_images = support_images.to(train_device)\n    support_labels = support_labels.to(train_device)\n    query_images = query_images.to(train_device)\n    query_labels = query_labels.to(train_device)\n\n    # Forward pass - Model handles Q/DQ stubs internally during training\n    classification_scores = model_to_train(support_images, support_labels, query_images)\n\n    # Calculate loss and update weights\n    loss = criterion(classification_scores, query_labels)\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n\n# --- Helper function to create and prepare a model instance for QAT ---\ndef create_and_prepare_qat_model(pretrained_fsl_weights_path=None, qat_device=QAT_DEVICE):\n    \"\"\"\n    Creates a Prototypical Network with a quantization-ready ResNet18 backbone,\n    loads optional pre-trained FSL weights, and prepares it for QAT.\n    \"\"\"\n    print(\"\\n--- Creating and Preparing Model for QAT ---\")\n    # 1. Create Quantization-Ready Backbone\n    #    Use torchvision.models.quantization.resnet18\n    #    quantize=False indicates it's ready for QAT, not static quantization.\n    #    Load standard ImageNet weights as a starting point.\n    print(\"Creating quantization-ready ResNet18 backbone (quantize=False)...\")\n    qat_backbone = quantized_resnet18(weights='IMAGENET1K_V1', quantize=False)\n    # Replace the final fully connected layer (classifier) with Identity,\n    # as we only need the features before the classification head.\n    qat_backbone.fc = nn.Identity()\n\n    # 2. Create the Full Prototypical Network Model\n    print(\"Creating PrototypicalNetworks model with QAT backbone...\")\n    # Note: Don't move to device yet, weights loading might specify map_location\n    model = PrototypicalNetworks(qat_backbone)\n\n    # 3. Load Pre-trained FSL Weights (if provided and exist)\n    if pretrained_fsl_weights_path and os.path.exists(pretrained_fsl_weights_path):\n        try:\n            print(f\"Loading FSL state dict from: {pretrained_fsl_weights_path}\")\n            # Load state dict to CPU first for flexibility\n            state_dict = torch.load(pretrained_fsl_weights_path, map_location='cpu')\n\n            # --- Critical Step for loading weights into QAT model ---\n            # We need strict=False because:\n            #   a) The original model (likely standard ResNet) doesn't have Quant/DeQuant stubs.\n            #   b) The backbone module keys might differ slightly if the original wasn't\n            #      explicitly using the `torchvision.models.quantization` variant.\n            # Filter warnings related to unexpected keys if they occur during loading\n            with warnings.catch_warnings():\n                 warnings.simplefilter(\"ignore\", category=UserWarning) # Often warns about size mismatches if fc differs\n                 model.load_state_dict(state_dict, strict=False)\n            print(\"Pre-trained FSL weights loaded successfully into QAT structure (strict=False).\")\n            # ---\n        except Exception as e:\n            print(f\"Warning: Error loading pre-trained FSL weights: {e}. Check compatibility. Using ImageNet weights only for backbone.\")\n    elif pretrained_fsl_weights_path:\n        print(f\"Warning: Pre-trained FSL weights file not found at {pretrained_fsl_weights_path}. Using ImageNet weights only for backbone.\")\n    else:\n        print(\"No FSL pre-trained weights path provided. Using ImageNet weights only for backbone.\")\n\n    # 4. Move model to the designated QAT device *after* loading weights\n    model.to(qat_device)\n\n    # 5. Configure and Prepare for QAT\n    model.train() # QAT preparation requires model to be in training mode\n\n    # Select backend ('fbgemm' for x86, 'qnnpack' for ARM). Default usually works.\n    # Using get_default_qat_qconfig is generally recommended.\n    backend = \"qnnpack\" if torch.backends.quantized.engine == \"qnnpack\" else \"fbgemm\"\n    print(f\"Using quantization backend: {backend}\")\n    model.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n    print(f\"Applied default QAT qconfig for backend {backend}.\")\n\n    # --- Fusion Step (Generally NOT needed for torchvision.models.quantization) ---\n    # The quantized ResNet variants are typically pre-fused or structured correctly.\n    # Explicit fusion here might be redundant or cause issues.\n    # print(\"Skipping explicit fusion step; relying on pre-structured quantized ResNet.\")\n    # ---\n\n    # 6. Prepare the model for QAT\n    # This inserts observers into the model to collect activation statistics during training.\n    print(\"Applying torch.quantization.prepare_qat...\")\n    # Ensure model is in training mode *before* calling prepare_qat\n    model.train()\n    # inplace=True modifies the model directly\n    torch.quantization.prepare_qat(model, inplace=True)\n    print(\"Model prepared successfully for QAT.\")\n\n    return model\n\n# --- Main Execution Logic ---\ndef main():\n    # 1. Download Pre-trained FSL Weights (Optional but recommended)\n    if not os.path.exists(PRETRAINED_FSL_WEIGHTS_FILE):\n        print(f\"Downloading pre-trained FSL weights to {PRETRAINED_FSL_WEIGHTS_FILE}...\")\n        try:\n            # Use wget or curl via subprocess\n            subprocess.run([\"wget\", \"-O\", PRETRAINED_FSL_WEIGHTS_FILE, PRETRAINED_FSL_WEIGHTS_URL], check=True, timeout=120)\n            print(\"Download complete.\")\n        except FileNotFoundError:\n             print(\"Error: 'wget' command not found. Please download the weights manually:\")\n             print(f\"URL: {PRETRAINED_FSL_WEIGHTS_URL}\")\n             print(f\"Save as: {PRETRAINED_FSL_WEIGHTS_FILE}\")\n             return # Exit if weights are needed but download fails\n        except subprocess.CalledProcessError as e:\n            print(f\"Error during download (wget returned non-zero exit status {e.returncode}).\")\n            return\n        except subprocess.TimeoutExpired:\n            print(\"Error: Download timed out.\")\n            return\n        except Exception as e:\n            print(f\"An unexpected error occurred during download: {e}\")\n            return\n    else:\n        print(f\"Pre-trained FSL weights file found: {PRETRAINED_FSL_WEIGHTS_FILE}\")\n    fsl_weights_path = PRETRAINED_FSL_WEIGHTS_FILE if os.path.exists(PRETRAINED_FSL_WEIGHTS_FILE) else None\n\n    # --- Variables to store accuracies and losses for plotting ---\n    fp32_acc = 0.0\n    qat_pre_conversion_acc = 0.0\n    int8_acc = 0.0\n    int8_reloaded_acc = 0.0\n    training_losses = []\n\n\n    # --- Optional: Evaluate Original FP32 Model (for baseline comparison) ---\n    print(\"\\n--- Evaluating Original FP32 Model (Reference) ---\")\n    try:\n        # Create standard ResNet18 backbone\n        ref_backbone = standard_resnet18(weights='IMAGENET1K_V1')\n        ref_backbone.fc = nn.Identity()\n        ref_model = PrototypicalNetworks(ref_backbone)\n\n        if fsl_weights_path:\n            print(f\"Loading FSL weights into FP32 reference model...\")\n            ref_state_dict = torch.load(fsl_weights_path, map_location='cpu')\n            # Use strict=False here too, as the PrototypicalNetworks wrapper adds quant/dequant\n            # even if the backbone itself is standard. The keys won't perfectly match.\n            with warnings.catch_warnings():\n                 warnings.simplefilter(\"ignore\", category=UserWarning)\n                 ref_model.load_state_dict(ref_state_dict, strict=False)\n            print(\"Loaded FSL weights into reference model.\")\n        else:\n             print(\"Skipping FSL weight loading for reference model (file not found/download failed).\")\n\n        ref_model.to(DEVICE) # Evaluate reference model on the primary device (GPU if available)\n        fp32_acc = evaluate(test_loader, ref_model, description=\"FP32 Reference Eval\", eval_device=DEVICE)\n\n        # Clean up GPU memory if used\n        del ref_model\n        del ref_backbone\n        if DEVICE == 'cuda': torch.cuda.empty_cache()\n\n    except Exception as e:\n        print(f\"Could not evaluate reference FP32 model: {e}\")\n        # Ensure cleanup even on error\n        if 'ref_model' in locals(): del ref_model\n        if 'ref_backbone' in locals(): del ref_backbone\n        if DEVICE == 'cuda': torch.cuda.empty_cache()\n    # --- End Optional FP32 Eval ---\n\n\n    # 2. Create and Prepare Model for QAT\n    qat_model = create_and_prepare_qat_model(pretrained_fsl_weights_path=fsl_weights_path, qat_device=QAT_DEVICE)\n\n    # 3. Perform Quantization Aware Training (Fine-tuning)\n    print(\"\\n--- Starting Quantization Aware Training (Fine-tuning) ---\")\n    # Use a smaller learning rate for fine-tuning QAT\n    optimizer = optim.Adam(qat_model.parameters(), lr=LEARNING_RATE)\n    criterion = nn.CrossEntropyLoss()\n\n    all_loss = []\n    qat_model.to(QAT_DEVICE) # Ensure model is on the correct device for training\n\n    with tqdm(train_loader, total=len(train_loader), desc=\"QAT Training\") as tqdm_train:\n        for episode_index, (support_images, support_labels, query_images, query_labels, _) in enumerate(tqdm_train):\n            loss_value = fit_one_task(\n                qat_model, optimizer, criterion,\n                support_images, support_labels, query_images, query_labels,\n                train_device=QAT_DEVICE\n            )\n            all_loss.append(loss_value)\n            training_losses.append(loss_value) # Store loss for plotting\n\n            # Log average loss periodically\n            if episode_index % LOG_UPDATE_FREQUENCY == 0 and episode_index > 0:\n                 avg_interval = min(len(all_loss), LOG_UPDATE_FREQUENCY * 2) # Use available history\n                 if avg_interval > 0:\n                    avg_loss = sum(all_loss[-avg_interval:]) / avg_interval\n                    tqdm_train.set_postfix(loss=f\"{avg_loss:.4f}\")\n\n    print(\"QAT Fine-tuning finished.\")\n\n    # --- Optional: Evaluate QAT model *before* conversion ---\n    # This evaluates the model with observers active, still using QAT device\n    print(\"\\n--- Evaluating QAT Model (Before Conversion) ---\")\n    qat_pre_conversion_acc = evaluate(test_loader, qat_model, description=\"QAT Pre-Conversion Eval\", eval_device=QAT_DEVICE)\n    # ---\n\n    # 4. Save the QAT Model State (Weights + Observers)\n    print(f\"\\n--- Saving QAT model state (including observers) to {QAT_STATE_DICT_FILENAME} ---\")\n    # Important: Save the state dict while the model includes observers, before conversion.\n    # Move to CPU before saving for better compatibility.\n    qat_model.eval() # Set to eval mode\n    qat_model_state_dict_cpu = qat_model.to('cpu').state_dict()\n    torch.save(qat_model_state_dict_cpu, QAT_STATE_DICT_FILENAME)\n    print(\"QAT model state saved.\")\n    # Clean up GPU memory if QAT was done there\n    del qat_model\n    if QAT_DEVICE == 'cuda': torch.cuda.empty_cache()\n\n\n    # 5. Convert the Model to Quantized INT8\n    print(\"\\n--- Converting Model to Final Quantized INT8 Format ---\")\n    # A. Create a fresh instance of the model prepared for QAT (on CPU)\n    #    This ensures the architecture exactly matches the one used for QAT.\n    #    Do not load FSL weights here; they are part of the QAT state dict.\n    model_to_convert = create_and_prepare_qat_model(pretrained_fsl_weights_path=None, qat_device='cpu')\n\n    # B. Load the saved QAT state dict (weights + observers) into this fresh instance\n    print(f\"Loading saved QAT state from: {QAT_STATE_DICT_FILENAME}\")\n    model_to_convert.load_state_dict(torch.load(QAT_STATE_DICT_FILENAME, map_location='cpu'))\n    print(\"QAT state loaded successfully into conversion model.\")\n\n    # C. Convert the model\n    model_to_convert.eval() # Ensure model is in evaluation mode for conversion\n    model_to_convert.to('cpu') # Conversion typically happens on CPU\n    print(\"Applying torch.quantization.convert...\")\n    # inplace=False creates a new converted model (safer)\n    quantized_int8_model = torch.quantization.convert(model_to_convert, inplace=False)\n    print(\"Model successfully converted to INT8.\")\n    # Clean up the pre-conversion model\n    del model_to_convert\n    torch.cuda.empty_cache() # Just in case\n\n    # 6. Save the Final Quantized INT8 Model State Dict\n    print(f\"\\n--- Saving final INT8 quantized model state dict to {FINAL_INT8_STATE_DICT_FILENAME} ---\")\n    quantized_int8_model.eval() # Ensure eval mode\n    # Already on CPU from conversion step\n    torch.save(quantized_int8_model.state_dict(), FINAL_INT8_STATE_DICT_FILENAME)\n    print(\"Final INT8 model state dict saved.\")\n\n\n    # 7. Evaluate the Final Quantized INT8 Model (on CPU)\n    print(\"\\n--- Evaluating Final Quantized INT8 Model ---\")\n    int8_acc = evaluate(test_loader, quantized_int8_model, description=\"Final INT8 Quantized Eval\", eval_device=EVAL_DEVICE) # EVAL_DEVICE is CPU\n\n\n    # --- How to load the final INT8 model later ---\n    print(\"\\n--- Example: How to load and use the final INT8 model later ---\")\n    # 1. Create the *quantized* model architecture instance.\n    #    Start with the QAT-ready structure, as the saved state dict keys match that.\n    print(\"Creating base QAT-ready structure (on CPU)...\")\n    final_model_structure = create_and_prepare_qat_model(pretrained_fsl_weights_path=None, qat_device='cpu')\n    final_model_structure.eval()\n    # 2. Convert this structure to INT8 *before* loading the state dict.\n    #    This makes its layers expect quantized weights/biases.\n    print(\"Converting empty structure to INT8...\")\n    final_model_quantized_empty = torch.quantization.convert(final_model_structure, inplace=False)\n    # 3. Load the saved INT8 state dict.\n    print(f\"Loading saved INT8 state from: {FINAL_INT8_STATE_DICT_FILENAME}\")\n    final_model_quantized_empty.load_state_dict(torch.load(FINAL_INT8_STATE_DICT_FILENAME, map_location='cpu'))\n    print(\"Final INT8 state loaded.\")\n    # 4. The model `final_model_quantized_empty` is now ready for inference on CPU.\n    # Example: Evaluate it again to confirm loading worked\n    int8_reloaded_acc = evaluate(test_loader, final_model_quantized_empty, description=\"Reloaded Final INT8 Eval\", eval_device=EVAL_DEVICE)\n    # ---\n\n\n    print(\"\\nScript finished successfully.\")\n\n    # --- Plotting Code ---\n    print(\"\\n--- Generating Plots ---\")\n    # 1. Accuracy Comparison Bar Chart\n    accuracies = [fp32_acc, qat_pre_conversion_acc, int8_acc, int8_reloaded_acc]\n    model_names = ['FP32 Ref', 'QAT Pre-Conv', 'INT8', 'INT8 Reloaded']\n    plt.figure(figsize=(8, 6))\n    plt.bar(model_names, accuracies, color=['blue', 'green', 'red', 'purple'])\n    plt.ylabel('Accuracy (%)')\n    plt.title('Accuracy Comparison of Different Model Stages')\n    plt.ylim([min(accuracies)-1, max(accuracies)+1]) # Adjust y-axis limits to data range\n    plt.savefig(os.path.join(GRAPHS_DIR, \"accuracy_comparison.png\"))\n    plt.close()\n    print(f\"Accuracy comparison plot saved to {GRAPHS_DIR}/accuracy_comparison.png\")\n\n    # 2. Training Loss Curve\n    plt.figure(figsize=(10, 6))\n    plt.plot(training_losses)\n    plt.xlabel('Training Episodes')\n    plt.ylabel('Loss')\n    plt.title('QAT Training Loss Curve')\n    plt.grid(True)\n    plt.savefig(os.path.join(GRAPHS_DIR, \"qat_training_loss.png\"))\n    plt.close()\n    print(f\"QAT Training Loss plot saved to {GRAPHS_DIR}/qat_training_loss.png\")\n    print(\"--- Plots generated successfully in the 'graphs' directory ---\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T11:12:09.013507Z","iopub.execute_input":"2025-04-05T11:12:09.013853Z","iopub.status.idle":"2025-04-05T11:14:38.617589Z","shell.execute_reply.started":"2025-04-05T11:12:09.013827Z","shell.execute_reply":"2025-04-05T11:14:38.616540Z"}},"outputs":[{"name":"stdout","text":"Using QAT device: cuda\nUsing final evaluation device: cpu\nLoading Omniglot dataset...\nSetting up data loaders...\nData loading complete.\nPre-trained FSL weights file found: ./models/resnet18_with_pretraining.tar\n\n--- Evaluating Original FP32 Model (Reference) ---\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-22-39bd8801c7c1>:434: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ref_state_dict = torch.load(fsl_weights_path, map_location='cpu')\n","output_type":"stream"},{"name":"stdout","text":"Loading FSL weights into FP32 reference model...\nLoaded FSL weights into reference model.\n","output_type":"stream"},{"name":"stderr","text":"FP32 Reference Eval: 100%|██████████| 500/500 [00:15<00:00, 31.58it/s, acc=96.50%]\n","output_type":"stream"},{"name":"stdout","text":"FP32 Reference Eval complete. Accuracy: 96.50% (24124/25000) on cuda\n\n--- Creating and Preparing Model for QAT ---\nCreating quantization-ready ResNet18 backbone (quantize=False)...\nCreating PrototypicalNetworks model with QAT backbone...\nLoading FSL state dict from: ./models/resnet18_with_pretraining.tar\nPre-trained FSL weights loaded successfully into QAT structure (strict=False).\nUsing quantization backend: fbgemm\nApplied default QAT qconfig for backend fbgemm.\nApplying torch.quantization.prepare_qat...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-22-39bd8801c7c1>:339: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(pretrained_fsl_weights_path, map_location='cpu')\n/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model prepared successfully for QAT.\n\n--- Starting Quantization Aware Training (Fine-tuning) ---\n","output_type":"stream"},{"name":"stderr","text":"QAT Training: 100%|██████████| 1000/1000 [01:00<00:00, 16.57it/s, loss=0.0320]\n","output_type":"stream"},{"name":"stdout","text":"QAT Fine-tuning finished.\n\n--- Evaluating QAT Model (Before Conversion) ---\n","output_type":"stream"},{"name":"stderr","text":"QAT Pre-Conversion Eval: 100%|██████████| 500/500 [00:19<00:00, 26.27it/s, acc=98.14%]\n","output_type":"stream"},{"name":"stdout","text":"QAT Pre-Conversion Eval complete. Accuracy: 98.14% (24536/25000) on cuda\n\n--- Saving QAT model state (including observers) to ./models/qat_proto_omniglot_state_1743851529.pth ---\nQAT model state saved.\n\n--- Converting Model to Final Quantized INT8 Format ---\n\n--- Creating and Preparing Model for QAT ---\nCreating quantization-ready ResNet18 backbone (quantize=False)...\nCreating PrototypicalNetworks model with QAT backbone...\nNo FSL pre-trained weights path provided. Using ImageNet weights only for backbone.\nUsing quantization backend: fbgemm\nApplied default QAT qconfig for backend fbgemm.\nApplying torch.quantization.prepare_qat...\nModel prepared successfully for QAT.\nLoading saved QAT state from: ./models/qat_proto_omniglot_state_1743851529.pth\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-22-39bd8801c7c1>:520: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_to_convert.load_state_dict(torch.load(QAT_STATE_DICT_FILENAME, map_location='cpu'))\n","output_type":"stream"},{"name":"stdout","text":"QAT state loaded successfully into conversion model.\nApplying torch.quantization.convert...\nModel successfully converted to INT8.\n\n--- Saving final INT8 quantized model state dict to ./models/final_int8_proto_omniglot_state_1743851529.pth ---\nFinal INT8 model state dict saved.\n\n--- Evaluating Final Quantized INT8 Model ---\n","output_type":"stream"},{"name":"stderr","text":"Final INT8 Quantized Eval: 100%|██████████| 500/500 [00:25<00:00, 19.78it/s, acc=97.58%]\n","output_type":"stream"},{"name":"stdout","text":"Final INT8 Quantized Eval complete. Accuracy: 97.58% (24395/25000) on cpu\n\n--- Example: How to load and use the final INT8 model later ---\nCreating base QAT-ready structure (on CPU)...\n\n--- Creating and Preparing Model for QAT ---\nCreating quantization-ready ResNet18 backbone (quantize=False)...\nCreating PrototypicalNetworks model with QAT backbone...\nNo FSL pre-trained weights path provided. Using ImageNet weights only for backbone.\nUsing quantization backend: fbgemm\nApplied default QAT qconfig for backend fbgemm.\nApplying torch.quantization.prepare_qat...\nModel prepared successfully for QAT.\nConverting empty structure to INT8...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:407: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n  warnings.warn(\n<ipython-input-22-39bd8801c7c1>:560: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  final_model_quantized_empty.load_state_dict(torch.load(FINAL_INT8_STATE_DICT_FILENAME, map_location='cpu'))\n","output_type":"stream"},{"name":"stdout","text":"Loading saved INT8 state from: ./models/final_int8_proto_omniglot_state_1743851529.pth\nFinal INT8 state loaded.\n","output_type":"stream"},{"name":"stderr","text":"Reloaded Final INT8 Eval: 100%|██████████| 500/500 [00:25<00:00, 19.38it/s, acc=97.25%]\n","output_type":"stream"},{"name":"stdout","text":"Reloaded Final INT8 Eval complete. Accuracy: 97.25% (24313/25000) on cpu\n\nScript finished successfully.\n\n--- Generating Plots ---\nAccuracy comparison plot saved to ./graphs/accuracy_comparison.png\nQAT Training Loss plot saved to ./graphs/qat_training_loss.png\n--- Plots generated successfully in the 'graphs' directory ---\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}